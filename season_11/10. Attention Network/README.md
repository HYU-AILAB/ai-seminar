# [Title]

## Paper

- 키워드 : seq2seq, Attention, Self-Attention
- 한줄 소개 : Attention의 개념과 구현을 알아보고, 나아가 Self-Attention까지 이해해본다.

## References

* Paper
  1. Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. "Neural machine translation by jointly learning to align and translate." arXiv preprint arXiv:1409.0473 (2014).
  2. Sutskever, Ilya, Oriol Vinyals, and Quoc V. Le. "Sequence to sequence learning with neural networks." Advances in neural information processing systems. 2014.
  3. Vaswani, Ashish, et al. "Attention is all you need." Advances in neural information processing systems. 2017.
* Blog
  1. “Dissecting BERT Part 1: The Encoder https://medium.com/dissecting-bert/dissecting-bert-part-1-d3c3d495cdb3
  2. “Attention mechanism in NLP. From seq2seq + attention to BERT https://lovit.github.io/machine%20learning/2019/03/17/attention_in_nlp/
* Youtube
  1. “십분딥러닝_12_어텐션(Attention Mechanism)” https://www.youtube.com/watch?v=6aouXD8WMVQ

