# Title
### Supervised Contrastive Learning For Pre-trained Language Model Fine-tuning
## Paper

- link : https://arxiv.org/abs/2011.01403
- 키워드 : Contrastive Learning
- 한줄 소개 : Cross-Entropy Loss와 Contrastive Loss를 같이 쓰면 NLU task에서 성능이 다방면으로 좋아진다.

### References

 - **_Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning 리뷰, HOYA012’S RESEARCH BLOG, https://hoya012.github.io/blog/byol/

 - **_PR-231: A Simple Framework for Contrastive Learning of Visual Representations, JinWon Lee, https://youtu.be/FWhM3juUM6s

- **_Self-Supervised Learning(Algorithm&application), Seokho Moon, file:///C:/Users/wonhyuk/Desktop/20201120_Self_Supervised_Representation_Learning_Seokho.pdf

- **_Chen, Ting, et al. "A simple framework for contrastive learning of visual representations." International conference on machine learning. PMLR, 2020.

- **_Gunel, Beliz, et al. "Supervised Contrastive Learning for Pre-trained Language Model Fine-tuning." arXiv preprint arXiv:2011.01403 (2020).

- **_Temperature Scaling, Calibration: On Calibration of Modern Neural Netwoks, Curaai00’s Deep Learning Blog, https://curaai00.tistory.com/10

- **_The Illustrated SimCLR Framework, Amit Chaudhary, https://amitness.com/2020/03/illustrated-simclr/

- **_SimCLR을 이용한 향상된 자기주도 및 반주도 학습, 시나브로의 테크산책, https://brunch.co.kr/@synabreu/76

