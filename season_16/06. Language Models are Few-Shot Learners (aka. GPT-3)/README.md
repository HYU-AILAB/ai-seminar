# Title
- Language Models are Few-Shot Learners (aka. GPT-3)

## Paper

- link : https://arxiv.org/pdf/2005.14165.pdf
- 키워드 : Transformer, Language Modeling, NLP Task, Huge Parameter Model, few-shot, zero-shot
- 한줄 소개 : Language Model pre-train을 진행한 거대한 모델이(175B Parameters) 다양한 NLP Task에서 fine-tuning 없이 좋은 성능을 낼 수 있다.
