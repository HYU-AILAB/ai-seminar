# title
- BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension
## Paper

- link : https://arxiv.org/abs/1910.13461

- 키워드 : ACL2020, seq2seq

- 한줄 소개 : 

### References

- 


