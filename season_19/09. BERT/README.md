# BERT

## Paper

- link : https://arxiv.org/pdf/1810.04805.pdf
- 키워드 : Pre-trained deep bidirectional representations 
- 한줄 소개 : 

## References

https://jalammar.github.io/illustrated-word2vec/
https://wikidocs.net/108730
https://nlp.seas.harvard.edu/2018/04/03/attention.html 
https://github.com/SKTBrain/KoBERT#naver-sentiment-analysis

