# BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding

## Paper

- link : https://arxiv.org/abs/1810.04805
- 키워드 :
- 한줄 소개 : 

## References

- Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., & Polosukhin, I. (2017). Attention is all you need. Advances in Neural Information Processing Systems, 2017-Decem(Nips), 5999–6009.
- Devlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2018). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Mlm.
- "Season 07 - 05. Attention Is All You Need." HYU-AILAB/ai-seminar. 2020년 4월 13일 접속, https://github.com/HYU-AILAB/ai-seminar/tree/master/season_07/05.%20Attention%20is%20All%20you%20Need.
- "Season 04 - 09. [NLP] BERT (Pre-training of Deep Bidirectional Transformers for Language Understanding)." HYU-AILAB/ai-seminar. 2020년 4월 13일 접속, https://github.com/HYU-AILAB/ai-seminar/tree/master/season_04/09.%20%5BNLP%5D%20BERT%20(Pre-training%20of%20Deep%20Bidirectional%20Transformers%20for%20Language%20Understanding).
- "Transformer: All you need is Attention (설명/요약/정리)." 만렙개발자. 2019년 12월 31일 수정, 2020년 4월 13일 접속, https://lv99.tistory.com/26.
- "BERT 논문정리." Mino-Park7 NLP Blog. 2018년 12월 12일 수정, 2020년 4월 13일 접속, https://mino-park7.github.io/nlp/2018/12/12/bert-논문정리/.
- "인공지능(AI) 언어모델 'BERT(버트)'는 무엇인가." 인공지능신문. 2019년 1월 3일 수정, 2020년 4월 13일 접속, http://www.aitimes.kr/news/articleView.html?idxno=13117.
- "BERT 톺아보기." The Missing Papers. 2019년 1월 24일 수정, 2020년 4월 13일 접속, http://docs.likejazz.com/bert/.
- "The Illustrated Transformer." NLP in Korean. 2018년 12월 20일 수정, 2020년 4월 13일 접속, https://nlpinkorean.github.io/illustrated-transformer/.
- "Transformer: Attention Is All You Need 리뷰." Huiwon. 2018년 12월 6일 수정, 2020년 4월 13일 접속, https://vanche.github.io/NLP_Transformer/.
- "Attention is all you need paper 뽀개기." 포자랩스의 기술 블로그. 2018년 9월 15일 수정, 2020년 4월 13일 접속, https://pozalabs.github.io/transformer/.
