# title
**RoBERTa: A Robustly Optimized BERT Pretraining Approach**
## Paper

- link : [Paper](https://arxiv.org/pdf/1907.11692.pdf)

- 키워드 : Pre-training, BERT, Masked Language Model, Next Sentence Prediction, replication study of BERT pretraining

- 한줄 소개 : BERT pretraining 대한 replication study

### References

- [Huiwon's blog](https://vanche.github.io/spanbert_roberta/)
- [Jeong Ukjae Engineering blog](https://jeongukjae.github.io/posts/3-roberta-review/)
- [Yeongmin's Blog](https://baekyeongmin.github.io/paper-review/roberta-review/)


